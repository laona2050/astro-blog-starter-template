---
name: MongoDB
slug: mongodb
version: 1.0.1
description: Design MongoDB schemas with proper embedding, indexing, aggregation, and production-ready patterns.
metadata: {"clawdbot":{"emoji":"ðŸƒ","requires":{"anyBins":["mongosh","mongo"]},"os":["linux","darwin","win32"]}}
---

## When to Use

User needs MongoDB expertise â€” from schema design to production optimization. Agent handles document modeling, indexing strategies, aggregation pipelines, consistency patterns, and scaling.

## Quick Reference

| Topic | File |
|-------|------|
| Schema design patterns | `schema.md` |
| Index strategies | `indexes.md` |
| Aggregation pipeline | `aggregation.md` |
| Production configuration | `production.md` |

## Schema Design Philosophy

- Embed when data is queried together and doesn't grow unboundedly
- Reference when data is large, accessed independently, or many-to-many
- Denormalize for read performance, accept update complexityâ€”no JOINs means duplicate data
- Design for your queries, not for normalized elegance

## Document Size Traps

- 16MB max per documentâ€”plan for this from day one; use GridFS for large files
- Arrays that grow infinitely = disasterâ€”use bucketing pattern instead
- BSON overhead: field names repeated per documentâ€”short names save space at scale
- Nested depth limit 100 levelsâ€”rarely hit but exists

## Array Traps

- Arrays > 1000 elements hurt performanceâ€”pagination inside documents is hard
- `$push` without `$slice` = unbounded growth; use `$push: {$each: [...], $slice: -100}`
- Multikey indexes on arrays: index entry per elementâ€”can explode index size
- Can't have multikey index on more than one array field in compound index

## $lookup Traps

- `$lookup` performance degrades with collection sizeâ€”no index on foreign collection (until 5.0)
- One `$lookup` per pipeline stageâ€”nested lookups get complex and slow
- `$lookup` with pipeline (5.0+) can filter before joiningâ€”massive improvement
- Consider: if you $lookup frequently, maybe embed instead

## Index Strategy

- ESR rule: Equality fields first, Sort fields next, Range fields last
- MongoDB doesn't do efficient index intersectionâ€”single compound index often better
- Only one text index per collectionâ€”plan carefully; use Atlas Search for complex text
- TTL index for auto-expiration: `{createdAt: 1}, {expireAfterSeconds: 86400}`

## Consistency Traps

- Default read/write concern not fully consistentâ€”`{w: "majority", readConcern: "majority"}` for strong
- Multi-document transactions since 4.0â€”but add latency and lock overhead; design to minimize
- Single-document operations are atomicâ€”exploit this by embedding related data
- `retryWrites: true` in connection stringâ€”handles transient failures automatically

## Read Preference Traps

- Stale reads on secondariesâ€”replication lag can be seconds
- `nearest` for lowest latencyâ€”but may read stale data
- Write always goes to primaryâ€”read preference doesn't affect writes
- Read your own writes: use `primary` or session-based causal consistency

## ObjectId Traps

- Contains timestamp: `ObjectId.getTimestamp()`â€”extract creation time without extra field
- Roughly time-orderedâ€”can sort by `_id` for creation order without createdAt
- Not randomâ€”predictable if you know creation time; don't rely on for security tokens

## Performance Mindset

- `explain("executionStats")` shows actual executionâ€”not just theoretical plan
- `totalDocsExamined` vs `nReturned` ratio should be ~1â€”otherwise index missing
- `COLLSCAN` in explain = full collection scanâ€”add appropriate index
- Covered queries: `IXSCAN` + `totalDocsExamined: 0`â€”all data from index

## Aggregation Philosophy

- Pipeline stages are transformationsâ€”think of data flowing through
- Filter early (`$match`), project early (`$project`)â€”reduce data volume ASAP
- `$match` at start can use indexes; `$match` after `$unwind` cannot
- Test complex pipelines stage by stageâ€”build incrementally

## Common Mistakes

- Treating MongoDB as "schemaless"â€”still need schema design; just enforced in app not DB
- Not adding indexesâ€”scans entire collection; every query pattern needs index
- Giant documents via array pushesâ€”hit 16MB limit or slow BSON parsing
- Ignoring write concernâ€”data may appear written but not persisted/replicated
